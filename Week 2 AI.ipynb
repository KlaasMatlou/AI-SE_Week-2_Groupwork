{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad53f13-79e8-461d-8ea6-86bba2cb7b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"Crop_recommendation (1).csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badaaebd-8e4c-43e1-9c11-61f1091d1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"Crop_recommendation (1).csv\")\n",
    "\n",
    "# Simulate crop failure: assume yield < threshold for some crops = failure (binary label)\n",
    "import numpy as np\n",
    "df['failure'] = np.where((df['rainfall'] < 60) | (df['ph'] < 5.5) | (df['humidity'] < 45), 1, 0)\n",
    "\n",
    "# Features and label\n",
    "X = df.drop(['label', 'failure'], axis=1)\n",
    "y = df['failure']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8d4d4",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff60fc-7c76-42b7-a9fe-32e6ae0a5a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335f189-d977-4ab4-b149-f6ca7298df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ab8b4-451c-498c-bc2f-9cd7a7548c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfebee56-e7a0-423a-be71-63aa8ab6bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating  performance on the test set\n",
    "# Predict on test set\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate TESTING performance\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "print(f\"Test F1-score: {test_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f00646",
   "metadata": {},
   "source": [
    "## SVM ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596dbdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train model\n",
    "# Replace LogisticRegression with SVC\n",
    "clf = SVC(kernel='rbf', random_state=42)  # Using Radial Basis Function kernel\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1-score:Â {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974540c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65cd66",
   "metadata": {},
   "source": [
    "# Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Algortihm... Logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train model, Replace RandomForestClassifier with LogisticRegression\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e3f03-db19-4317-a637-e7e3ff93c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffcfc5",
   "metadata": {},
   "source": [
    "# Handling Overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate on Training Data (Logistic Regression)\n",
    "print(\"\\nMetrics on Training Data (Logistic Regression):\")\n",
    "y_train_pred_lr = clf.predict(X_train) # Use the Logistic Regression model\n",
    "\n",
    "acc_train_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "prec_train_lr = precision_score(y_train, y_train_pred_lr)\n",
    "rec_train_lr = recall_score(y_train, y_train_pred_lr)\n",
    "f1_train_lr = f1_score(y_train, y_train_pred_lr)\n",
    "\n",
    "print(f\"Accuracy (Train): {acc_train_lr:.2f}\")\n",
    "print(f\"Precision (Train): {prec_train_lr:.2f}\")\n",
    "print(f\"Recall (Train): {rec_train_lr:.2f}\")\n",
    "print(f\"F1-score (Train): {f1_train_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Cross-Validation.... To see if it works consistently or was just lucky on the first test.\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Perform cross-validation (Logistic Regression)\n",
    "# We'll use 5 folds (cv=5) and evaluate accuracy, precision, recall, and f1-score\n",
    "print(\"\\nCross-Validation Results (5-fold for Logistic Regression):\")\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results_lr = cross_validate(clf, X_scaled, y, cv=5, scoring=scoring) # Use the Logistic Regression model\n",
    "\n",
    "# Print the average scores across all folds\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {cv_results_lr[f'test_{metric}'].mean():.2f} (+/- {cv_results_lr[f'test_{metric}'].std():.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ce100",
   "metadata": {},
   "source": [
    "# Handling Overfitting \n",
    "- Adding Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16789100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add regularization to the Logistic Regression model to see \n",
    "# To make it predict well on new stuff, not just the data it learned from\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train model\n",
    "# We will use Logistic Regression with regularization\n",
    "# Add regularization by setting the 'C' parameter\n",
    "# A smaller C means stronger regularization\n",
    "clf = LogisticRegression(random_state=42, C=0.1) # Example: reducing C to 0.1\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate on Training Data (Logistic Regression)\n",
    "print(\"\\nMetrics on Training Data (Logistic Regression):\")\n",
    "y_train_pred_lr = clf.predict(X_train) # Use the Logistic Regression model\n",
    "\n",
    "acc_train_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "prec_train_lr = precision_score(y_train, y_train_pred_lr)\n",
    "rec_train_lr = recall_score(y_train, y_train_pred_lr)\n",
    "f1_train_lr = f1_score(y_train, y_train_pred_lr)\n",
    "\n",
    "print(f\"Accuracy (Train): {acc_train_lr:.2f}\")\n",
    "print(f\"Precision (Train): {prec_train_lr:.2f}\")\n",
    "print(f\"Recall (Train): {rec_train_lr:.2f}\")\n",
    "print(f\"F1-score (Train): {f1_train_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Perform cross-validation (Logistic Regression)\n",
    "# We'll use 5 folds (cv=5) and evaluate accuracy, precision, recall, and f1-score\n",
    "print(\"\\nCross-Validation Results (5-fold for Logistic Regression):\")\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results_lr = cross_validate(clf, X_scaled, y, cv=5, scoring=scoring) # Use the Logistic Regression model\n",
    "\n",
    "# Print the average scores across all folds\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {cv_results_lr[f'test_{metric}'].mean():.2f} (+/- {cv_results_lr[f'test_{metric}'].std():.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3ae0f",
   "metadata": {},
   "source": [
    "# Handling Overfitting\n",
    "- Tuned Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the Logistic Regression model using Grid Search to find the best regularization parameter (C)\n",
    "# To see how good it is in a real situation.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]} # Example range of C values\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# We'll optimize for F1-score\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"\\nBest parameters found by Grid Search:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nBest cross-validation F1-score:\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_logreg_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set (optional)\n",
    "print(\"\\nMetrics on Test Data with Best Logistic Regression Model:\")\n",
    "y_pred_best_logreg = best_logreg_model.predict(X_test)\n",
    "\n",
    "acc_best_logreg = accuracy_score(y_test, y_pred_best_logreg)\n",
    "prec_best_logreg = precision_score(y_test, y_pred_best_logreg)\n",
    "rec_best_logreg = recall_score(y_test, y_pred_best_logreg)\n",
    "f1_best_logreg = f1_score(y_test, y_pred_best_logreg)\n",
    "\n",
    "print(f\"Accuracy: {acc_best_logreg:.2f}\")\n",
    "print(f\"Precision: {prec_best_logreg:.2f}\")\n",
    "print(f\"Recall: {rec_best_logreg:.2f}\")\n",
    "print(f\"F1-score: {f1_best_logreg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the training set to check for overfitting\n",
    "# To confirm it learned well but didn't just memorize everything (not overfitting)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define an expanded parameter grid to search\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 500, 1000]} # Expanded range\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter for convergence with larger C\n",
    "\n",
    "# Initialize GridSearchCV (optimizing for F1-score)\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"\\nBest parameters found by Grid Search (Expanded Range):\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"\\nBest cross-validation F1-score (Expanded Range):\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Get the best model\n",
    "best_logreg_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "print(\"\\nMetrics on Test Data with Best Tuned Logistic Regression Model:\")\n",
    "y_pred_best_logreg = best_logreg_model.predict(X_test)\n",
    "\n",
    "acc_best_logreg = accuracy_score(y_test, y_pred_best_logreg)\n",
    "prec_best_logreg = precision_score(y_test, y_pred_best_logreg)\n",
    "rec_best_logreg = recall_score(y_test, y_pred_best_logreg)\n",
    "f1_best_logreg = f1_score(y_test, y_pred_best_logreg)\n",
    "\n",
    "print(f\"Accuracy: {acc_best_logreg:.2f}\")\n",
    "print(f\"Precision: {prec_best_logreg:.2f}\")\n",
    "print(f\"Recall: {rec_best_logreg:.2f}\")\n",
    "print(f\"F1-score: {f1_best_logreg:.2f}\")\n",
    "\n",
    "# Evaluate the best model on the training set to check for overfitting\n",
    "print(\"\\nMetrics on Training Data with Best Tuned Logistic Regression Model:\")\n",
    "y_train_pred_best_logreg = best_logreg_model.predict(X_train)\n",
    "\n",
    "acc_train_best_logreg = accuracy_score(y_train, y_train_pred_best_logreg)\n",
    "prec_train_best_logreg = precision_score(y_train, y_train_pred_best_logreg)\n",
    "rec_train_best_logreg = recall_score(y_train, y_train_pred_best_logreg)\n",
    "f1_train_best_logreg = f1_score(y_train, y_train_pred_best_logreg)\n",
    "\n",
    "print(f\"Accuracy (Train): {acc_train_best_logreg:.2f}\")\n",
    "print(f\"Precision (Train): {prec_train_best_logreg:.2f}\")\n",
    "print(f\"Recall (Train): {rec_train_best_logreg:.2f}\")\n",
    "print(f\"F1-score (Train): {f1_train_best_logreg:.2f}\")\n",
    "\n",
    "\n",
    "# Display the confusion matrix for the best tuned model\n",
    "print(\"\\nConfusion Matrix for Best Tuned Logistic Regression Model:\")\n",
    "cm_best_logreg = confusion_matrix(y_test, y_pred_best_logreg)\n",
    "sns.heatmap(cm_best_logreg, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Tuned Logistic Regression)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a17b1",
   "metadata": {},
   "source": [
    "# Overfitting has been resolved through hyperparameter tuning.\n",
    "\n",
    " Based on the evaluation of the tuned Logistic Regression model with C=10:\n",
    "\n",
    "The performance metrics on the training data are very close to the metrics on the Unseen test data\n",
    "- (Accuracy: 0.88, Precision: 0.83, Recall: 0.86, F1-score: 0.85) \n",
    "- (Accuracy: 0.87, Precision: 0.80, Recall: 0.83, F1-score: 0.81).\n",
    "\n",
    "This small gap between training and test performance, along with the consistent cross-validation F1-score of 0.84, indicates that the model is not severely overfitting the training data.\n",
    "The hyperparameter tuning process, guided by cross-validation, successfully found a regularization strength (C=10) that allowed the model to learn the patterns in the data effectively without memorizing noise, thus addressing the potential for overfitting that might exist with different parameter settings or more complex models.\n",
    "The tuned Logistic Regression model demonstrates good generalization ability, achieving an F1-score of approximately 0.81 on unseen data, providing a reliable predictor for crop failure within the context of this dataset and synthetic label.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
